---
title: 'Fault tolerance'
summary: 'Fault tolerance summary'
publishedAt: '2022-04-25'
---

## Definitions

The terms *failure* and *fault* are key to any understanding of system reliability. Yet they are often misused.

Over time, failure has come to be defined in terms of specified service delivered by a system. This avoids circular definitions involving essentially synonymous terms such as *defect*. A system to have a **failure** if the service it delivers to the user deviates from compliance with the system specification for a specified period of time. While it may be difficult to arrive at an unambiguous specification of the service to be delivered by any system, the concept of an agreed-to specification is the most reasonable of the options for defining satisfactory service and the absence of satisfactory service, failure.

It is important to have some reference for the definition of failure, and the specification is a logical choice. The specification can be considered as a boundary to the system's [region of concern](#failure-regions). It is important to recognize that every system has an *explicit specification*, which is written, and an *implicit specification* that the system should at least behave as well as a reasonable person could expect based on experience with similar systems and with the world in general. Clearly, it is important to make as much of the specification as explicit as possible.

It has become the practice to define faults in terms of failures. The concept closest to the common understanding of the word fault is one that defines a **fault** as the adjudged cause of a failure. This fits with a common application of the verb form of the word fault, which involves determining cause or affixing blame. However, this requires an understanding of how failures are caused.

An alternate view of faults is to consider them failures in other systems that interact with the system under consideration:

* a subsystem internal to the system under consideration
* a component of the system under consideration
* an external system that interacts with the system under consideration
* the environment

In the first instance, the link between faults and failures is cause; in the second case it is level of abstraction or location.

The advantages of defining faults as failures of component/interacting systems are:

* one can consider faults without the need to establish a direct connection with a failure, so we can discuss faults that do not cause failures, i.e., the system is naturally fault tolerant
* the definition of a fault is the same as the definition of a failure with only the boundary of the relevant system or subsystem being different

This means that we can consider an obvious internal defect to be a fault without having to establish a causal relationship between the defect and a failure at the system boundary.

A fault can lead to:

* other faults
* failure
* nothing

A system with faults may continue to provide its service, that is, not fail. Such a system is said to be **fault tolerant**. Thus, an important motivation for differentiating between faults and failures is the need to describe the fault tolerance of a system. An observer inspecting the internals of the system would say that the faulty component had failed, because the observer's viewpoint is now at a lower level of detail.

The observable effect of a fault at the system boundary is called a [**symptom**](#symptoms). The most extreme symptom of a fault is a failure.

Note that a fault does not lead to a failure unless the result is observable by the user. This means that one person's fault is another person's failure.

The term **error** often is used in addition to the terms fault and failure. Often, errors are defined to be the result of faults, leading to failures. Informally, errors seem to be a passive concept associated with incorrect values in the system state. However, it is extremely difficult to develop unambiguous criteria for differentiating between faults and errors. Many researchers refer to value faults, which are also clearly erroneous values. The connection between error and failure is even more difficult to describe.

As we have seen, differentiation between failures and faults is essential for fault tolerant systems. A third term error, adds little to this distinction and can be a source of confusion.

> A fault is the initial root cause, which could be a hardware malfunction, a software bug, or a misconfiguration. A fault can produce abnormal behaviors referred to as errors (synonym of symptoms?), such as system call error return or Java exceptions. Some of the errors will have no user-visible side-effects or may be appropriately handled by software; other errors manifest into a failure, where the system malfunction is noticed by end users or operators [2].

## Requirements

To be fault tolerant, [a system](/notes/system) must be able to:

* detect
* diagnose
* confine
* mask
* compensate
* recover

## Degree

The degree of fault tolerance a system requires can be specified [quantitatively](#quantitative-goals) or [qualitatively](#qualitative-goals).

### Quantitative Goals

A quantitative reliability goal is usually expressed as the maximum allowed failure-rate. For example, the reliability figure usually stated as a goal for computer systems in commercial aircraft is less than 10<sup>-9</sup> failures per hour. The problem with stating reliability requirements in this manner is that it is difficult to know when it has been achieved. Standard statistical methods cannot be used to show such reliability with either standard or fault tolerant software. It is also clear that there is no way to achieve confidence that a system meets such a reliability goal through random testing. Nevertheless, reliability goals are often expressed in this manner.

### Qualitative Goals

An alternative method of specifying a system's reliability characteristics is to specify them qualitatively. Typical specifications would include next goals.

#### Fail-safe

Design the system so that, when it sustains a specified number of faults, it fails in a safe mode. For instance, railway signalling systems are designed to fail so that all trains stop.

#### Fail-op

Design the system so that, when it sustains a specified number of faults, it still provides a subset of its specified behavior.

#### No single point of failure

Design the system so that the failure of any single component will not cause the system to fail. Such systems are often designed so that the failed component can be replaced or repaired before another failure occurs.

#### Consistency

Design the system so that all information delivered by the system is equivalent to the information that would be delivered by an instance of a non-faulty system.

## Levels

There are three levels at which fault tolerance can be applied:

- [Hardware](#hardware-fault-tolerance)
- [Software](#software-fault-tolerance)
- [System](#system-fault-tolerance)

### Hardware fault tolerance

Traditionally, fault tolerance has been used to compensate for faults in computing resources. By managing extra hardware resources, the computer subsystem increases its ability to continue operation. Hardware fault tolerance measures include:

* redundant communications
* replicated processors
* additional memory
* redundant power supplies.

Hardware fault tolerance was particularly important in the early days of computing when the time between machine failures was measured in minutes.

### Software fault tolerance

The second level of fault tolerance recognizes that a fault-tolerant hardware platform does not, in itself, guarantee high availability to the system user. It is still important to structure the computer software to compensate for faults such as changes in programs or data structures due to transients or design errors. This is software fault tolerance. Mechanisms such as:

* [checkpoint](#checkpoint)
* [restart](#restart)
* [recovery blocks](#recovery-blocks)
* [multiple-version programs](#multiple-version-programs)

are often used at this level.

#### Checkpoint

#### Restart

#### Recovery blocks

#### Multiple-version programs

### System fault tolerance

At a third level, the computer subsystem may provide functions that compensate for failures in other system facilities that are not computer-based. This is system fault tolerance. For example, software can detect and compensate for failures in sensors. Measures at this level are usually application-specific. It is important that fault tolerance measures at all levels be compatible.

## Dependency relations

### Acyclic / Cyclic

A component of a system is said to **depend on another** component if the correctness of the first component's behavior requires the correct operation of the second component. Traditionally, the set of possible dependencies in a system are considered to form an acyclic graph. The term **fault tree** analysis seems to imply this, among other things. Indeed many systems exhibit this behavior, in which one fault leads to another which leads to another until eventually a failure occurs. It is possible, however, for a dependency relationship to cycle back upon itself. A dependency relationship is said to be **acyclic** if it forms part of a tree. A **cyclic** dependency relationship is one that cannot be described as part of a tree, but rather must be described as part of a directed cyclic graph.

### Failure regions

Defining a **failure region** limits the consideration of faults and failures to a portion of a system and its environment. This is necessary to insure that system specification, analysis and design efforts are concentrated on the portions of a system that can be observed and controlled by the designer and user. It helps to simplify an otherwise overwhelming task.

A system is typically made up of lots of components parts. These components are, in turn, made up of sub-components This continues arbitrarily until an **atomic component** (a component that is not divisible or that we choose not to divide into sub-components) is reached. Although all components are theoretically capable of having faults, for any system there is a level beyond which the faults are "not interesting". This level is called the **fault floor**. Atomic
components lie at the fault floor. **We are concerned with faults emerging from atomic components, but not faults that lie within these components**.

Similarly, as components are aggregated into a system, eventually the system is complete. Everything else (the user, the environment) is not a part of the system. This is the **system boundary**. **Failures occur when faults reach the system boundary**.

The **span of concern** begins at the boundaries between the system and the user and between the system and the environment, and ends at the fault floor. Faults below the fault floor are indistinguishable, either because they are not fully understood, or because they are too numerous. Informally, **the span of concern is the area within which faults are of interest**.

![](/images/notes/fault-tolerance/span-of-concern.png)

## Fault classes

No system can be made to tolerate all possible faults, so it is essential that the faults be considered throughout the requirements definition and system design process. However, it is impractical to enumerate all of the faults to be tolerated; faults must be aggregated into manageable **fault classes**.

Faults may be classified based on:

* [Locality](#locality)
* [Effect](#effect)
* [Cause](#immediate-cause)
* [Duration](#duration)
* Effect on System State

### Locality

#### Atomic Component Faults

A atomic component fault is a fault at the fault floor, that is, in a component that cannot be subdivided for analysis purposes.

#### Composite Component Faults

A composite component fault is one that arises within an aggregation of atomic components rather than in an atomic component. It may be the result of one or more atomic component faults.

#### System Level Faults

A system level fault is one that arises in the structure of a system rather than in the system's components. Such faults are usually interaction or integration faults, that is, they occur because of the way the system is assembled rather than because of the integrity of any individual component. Note that an inconsistency in the operating rules for a system may lead to a system level fault. System level faults also include operator faults, in which an operator does not correctly perform his or her role in system operation.

Systems that distribute objects or information are prone to a special kind of system fault: **replication faults**. Replication faults occur when replicated information in a system becomes inconsistent, either because replicates that
are supposed to provide identical results no longer do so, or because the aggregate of the data from the various replicates is no longer consistent with system specifications.

Replication faults can be caused by **malicious faults**, in which components such as processors "lie" by providing conflicting versions of the same information to other components in the system. Malicious faults are sometimes called **[Byzantine faults](https://en.wikipedia.org/wiki/Byzantine_fault)** after an early formulation of the problem in terms of Byzantine generals trying to reach a consensus on attacking when one of the generals is a traitor.

#### External Faults

External faults arise from outside the system boundary, the environment, or the user. **Environmental faults** include phenomena that directly affect the operation of the system, such as temperature, vibration, or nuclear or electromagnetic radiation or that affect the inputs provided to the system. **User faults** are created by the user in employing the system. Note that the roles of user and operator are considered separately; the user is considered to be external to the system while the operator is considered to be a part of the system.

### Effect

Faults may also be classified according to their effect on the user of the system or service. Since computer system components interact by exchanging data values in a specified time and/or sequence, fault effects can be cleanly separated into timing faults and value faults. Timing faults occur when a value is delivered before or after the specified time. Value faults occur when the data differs in value from the specification.

#### Value Faults

Computer systems communicate by providing values. A value fault occurs when a computation returns a result that does not meet the system's specification. Value faults are usually detected using knowledge of the allowable values of the data, possibly determined at run time.

#### Timing Faults

A timing fault occurs when a process or service is not delivered or completed within the specified time interval. Timing faults cannot occur if there is no explicit or implicit specification of a deadline. Timing faults can be detected by observing the time at which a required interaction takes place; no knowledge of the data involved is usually needed.

Since time increases monotonically, it is possible to further classify timing faults into:

* early
* late
* "never" (omission)

Since it is practically impossible to determine if "never" occurs, omission faults are really late timing faults that exceed an arbitrary limit. Systems that never produce value faults, but only fail by omission are called **fail-silent systems**. If all failures require system restart, the system is a **fail-stop system**.

### Duration

#### Persistent faults

Persistent faults remain active for a significant period of time. These faults are sometimes termed hard faults. Persistent faults usually are the easiest to detect and diagnose, but may be difficult to contain and mask unless redundant hardware is available. Persistent faults can be effectively detected by test routines that are interleaved with normal processing.

#### Transient faults

Transient faults remain active for a short period of time. Because of their short duration, transient faults are often detected through the faults that result from their propagation.

#### Periodic faults

A transient fault that becomes active periodically is a periodic fault (sometimes referred to as an *intermittent fault*).

### Immediate (*direct*) Cause

Faults can be classified according to the operational condition that causes them.

#### Resource depletion faults

Resource depletion faults occur when a portion of the system is unable to obtain the resources required to perform its task. Resources may include time on a processing or communications device, storage, power, logical structures such as a data structure, or a physical item such as a processor.

#### Logic faults

Logic faults occur when adequate resources are available, but the system does not behave according to specification. Logic faults may be the result of improper design or implementation. Logic faults may occur in hardware or software.

#### Physical faults

Physical faults occur when hardware breaks or a mutation occurs in executable software. Most common fault tolerance mechanisms deal with hardware faults.

### Ultimate (*original*) Cause

Faults can also be classified as to their ultimate cause. Ultimate causes are the things that must be fixed to eliminate a fault. These faults occur during the development process and are most effectively dealt with using [fault avoidance](/notes/dependable-system#fault-avoidance) and [fault removal](/notes/dependable-system#fault-removal) techniques.

#### Specification fault

A common ultimate cause of a fault is an improper requirements specification which leads to a specification fault. Technically this is not a fault, since a fault is defined to be the failure of a component/interacting systems and a failure is the deviation of the system from specification. However, it can be the reason a system deviates from the behavior expected by the user. An especially insidious instance of this arises when the requirements ignore aspects of the environment in which the system operates. For instance, radiation causing a bit to flip in a memory location would be a *value fault* which would be considered an *external fault*. However, if the fault propagates inside the system boundary the ultimate cause is a *specification fault* because the system specification did not foresee the problem.

#### Design fault

Flowing down the waterfall, a design fault results when the system design does not correctly match the requirements. The validation process is specifically designed to detect these faults.

#### Implementation fault

Flowing down the waterfall, an implementation fault arises when the system implementation does not adequately implement the design. The validation process is specifically designed to detect these faults.

#### Documentation fault

A documentation fault occurs when the documented system does not match the real system.

### Effect on System State

#### Crash

#### Amnesia

#### Partial amnesia

## Fault Attributes

### Observability

Faults originate in a system component or subsystem, in the system's environment, or in an interaction between the system and a user, operator, or another subsystem. A fault may ultimately have one of several effects:

* It may disappear with no perceptible effect
* It may remain in place with no perceptible effect
* It may lead to a sequence of additional faults that result in a failure in the system's delivered service (propagation to failure)
* It may lead to a sequence of additional faults with no perceptible effect on the system (undetected propagation)
* It may lead to a sequence of additional faults that have a perceptible effect on the system but do not result in a failure in the system's delivered service (detected propagation without failure)

Fault detection is usually the first step in fault tolerance. Even if other elements of a system prevent a failure by compensating for a fault, it is important to detect and remove faults to avoid the exhaustion of a systems fault tolerance resources.

A fault is **observable** if there is information about its existence available at the system interface. The information that indicates the existence of a fault is a **symptom**. A symptom may be a directly observed fault or failure, or it may be a change in system behavior such that the system still meets its specifications. A fault that a fault tolerance mechanism of a system has found is said to be **detected**. Otherwise it is **latent**, whether it is observable or not. The definition of detected is independent of whether or not the fault tolerance mechanism is able to successfully deal with the fault condition. For a fault to be detected, it is sufficient that it be known about.

### Propagation

A fault that propagates to other faults or failures is said to be **active**. A non-propagating fault is said to be **dormant**. When a previously dormant fault becomes active it is said to be **triggered**. An active fault may again become dormant, awaiting a new trigger. The sequence of faults, each successive one triggered by the preceding one and possibly ending in a failure, is known as a **fault trajectory**. Because of the ways faults trigger successive faults, a fault trajectory could be viewed as a chain reaction.

![](/images/notes/fault-tolerance/fault-attributes.png)

## Conditions for a failure

### Number of inputs

Minimum number of input events required to trigger the failures:

* `1` - 23%
* `2` - 50%
* `3` - 17%
* `4` - 5%
* `>4` - 5%

Of the 23% of failures that require only a single event to manifest, the event often involves rarely used or newly introduced features, or are caused by concurrency bugs.

### Types of input

Input events that led to failures. The `%` reports the percentage of failure where the input event is required to trigger the failure. Most failures require multiple preceding events, so the sum of the `%` is greater than 100%.

* [Starting a service](#starting-up-services) - 58%
* File/database write from client - 32%
* [Unreachable node](#unreachable-nodes) (network error, crash, etc.) - 24%
* [Configuration change](#configuration-changes) - 23%
* Adding a node to the running system - 15%
* File/database read from client - 13%
* Node restart (intentional) - 9%
* Data corruption - 3%
* Other - 4%

We consider these events to be "input events" from a testing and diagnostic point of view — some of the events (e.g., "unreachable node", "data corruption") are not strictly user inputs but can easily be emulated by a tester or testing tools. Note that many of the events have specific requirements for a failure to manifest (e.g., a "file write" event needs to occur on a particular data block), making the input event space to explore for testing immensely large.

#### Starting up services

More than half of the failures require the start of some services. This suggests that the starting of services — especially more obscure ones — should be more heavily tested. About a quarter of the failures triggered by starting a service occurred on systems that have been running for a long time; e.g., the HBase "Region Split" service is started only when a table grows larger than a threshold. While such a failure may seem hard to test since it requires a long running system, it can be exposed intentionally by forcing a start of the service during testing.

#### Unreachable nodes

24% of the failures occur because a node is unreachable. This is somewhat surprising given that network errors and individual node crashes are expected to occur regularly in large data centers. This suggests that tools capable of injecting network errors systematically should be used more extensively when inputing other events during testing.

#### Configuration changes

23% of the failures are caused by configuration changes. Of those, 30% involve misconfigurations. The remaining majority involve valid changes to enable certain features that may be rarelyused. While the importance of misconfigurations have been observed in previous studies, only a few techniques exist to automatically explore configurations changes and test the resulting reaction of the system. This suggests that testing tools should be extended to combine (both valid and invalid) configuration changes with other operations.

### Number of nodes

The production failures typically manifested themselves on configurations with a large number of nodes. This raises the question of how many nodes are required for an effective testing and debugging system.

Minimum number of nodes needed to trigger the failures:

* `1` - 37%
* `2` - 84%
* `3` - 98%
* `>3` - 100%

## Findings

### Finding 1

> A majority (77%) of the failures require more than one input event to manifest, but most of the failures (90%) require no more than 3.

### Finding 2

> The specific order of events is important in 88% of the failures that require multiple input events.

In most cases, a specific combination and sequence of multiple events is needed to transition the system into a failed state. Finding 1 and 2 show the complexity of failures in large distributed system. To expose the failures in testing, we need to not only explore the *combination* of multiple input events from an exceedingly large event space, we also need to explore different *permutations*.

### Finding 3

> Almost all (98%) of the failures are guaranteed to manifest on no more than 3 nodes. 84% will manifest on no more than 2 nodes.

Finding 3 implies that it is not necessary to have a large cluster to test for and reproduce failures.

Note that Finding 3 does not contradict the conventional wisdom that distributed system failures are more likely to manifest on large clusters. In the end, testing is a probabilistic exercise. A large cluster usually involves more diverse workloads and fault modes, thus increasing the chances for failures to manifest. However, what our finding suggests is that it is not necessary to have a large cluster of machines to expose bugs, as long as the specific sequence of input events occurs.

### Finding 4

A key question for testing and diagnosis is whether the failures are guaranteed to manifest if the required sequence of input events occur (i.e., **deterministic failures**), or not (i.e., **non-deterministic failures**)?

> 74% of the failures are deterministic — they are guaranteed to manifest given the right input event sequences.

This means that for a majority of the failures, we only need to explore the combination and permutation of input events, but no additional timing relationship. This is particularly meaningful for testing those failures that require long-running systems to manifest. As long as we can simulate those events which typically only occur on long running systems (e.g., region split in HBase typically only occurs when the region size grows too large), we can expose these deterministic failures. Moreover, the failures can still be reproduced after inserting additional log output, enabling tracing, or using debuggers.

### Finding 5

> Among the 51 non-deterministic failures, 53% have timing constraints only on the input events.

These constraints require an input event to occur either before or after some software internal execution event such as a procedure call.

### Finding 6

> 76% of the failures print explicit failurerelated error messages.

This finding somewhat contradicts the findings of our previous study on failures in non-distributed systems, including Apache httpd, PostgreSQL, SVN, squid, and GNU Coreutils, where only 43% of failures had explicit failure-related error messages logged. We surmise there are three possible reasons why developers output log messages more extensively for the distributed systems we studied:

* since distributed systems are more complex, and harder to debug, developers likely pay more attention to logging
* the horizontal scalability of these systems makes the performance overhead of outputing log message less critical
* communicating through message-passing provides natural points to log messages; for example, if two nodes cannot communicate with each other because of a network problem, both have the opportunity to log the error

### Finding 7

> For a majority (84%) of the failures, all of their triggering events are logged.

This suggests that it is possible to deterministically replay the majority of failures based on the existing log messages alone. Deterministic replay has been widely explored by the research community. However, these approaches are based on intrusive tracing with significant runtime overhead and the need to modify software/hardware.

### Finding 8

> Logs are noisy: the median of the number of log messages printed by each failure is 824.

This number was obtained when reproducing 73 of the 198 failures with a minimal configuration and using a minimal workload that is just sufficient to reproduce the failure. Moreover, we did not count the messages printed during the start-up and shut-down phases.

This suggests that manual examination of the log files could be tedious. If a user only cares about the error symptoms, a selective grep on the error verbosity levels will reduce noise since a vast majority of the printed log messages are at INFO level. However, the input events that triggered the failure are often logged at INFO level. Therefore to further infer the input events one has to examine almost every log message. It would be helpful if existing log analysis techniques and tools were extended so they can infer the relevant error and input event messages by filtering out the irrelevant ones.

### Finding 9

Conventional wisdom has it that failures which occur on large, distributed system in production are extremely hard to reproduce off-line. The users' input may be unavailable due to privacy concerns, the difficulty in setting up an environment that mirrors the one in production, and the cost of third-party libraries, are often reasons cited as to why it is difficult for vendors to reproduce production failures. Our finding below indicates that failure reproduction might not be as hard as it is thought to be.

> A majority of the production failures (77%) can be reproduced by a unit test.

While this finding might sound counter-intuitive, it is not surprising given our previous findings because:

* in Finding 4 we show that 74% of the failures are deterministic, which means the failures can be reproduced with the same operation sequence
* among the remaining non-deterministic failures, in 53% of the cases the timing can be controlled through unit tests

Specific data values are not typically required to reproduce the failures; in fact, *none* of the studied failures required specific values of user's data contents. Instead, only the required input sequences (e.g., file write, disconnect a node, etc.) are needed.

Figure 6 shows how a unit test can simulate the non-deterministic failure of Figure 2. It simulates a mini-cluster by starting three processes running as three nodes. It further simulates the key input events, including HMaster's log split, Region Server's log rolling, and the write requests. The required dependency where the client must send write requests before the master re-assigns the recovered region is also controlled by this unit test.

The failures that cannot be reproduced easily either depend on a particular execution environment (such as OS version or third party libraries), or were caused by nondeterministic thread interleavings.

### Finding 10

We classify a failure to be **catastrophic** when it prevents all or a majority of the users from their normal access to the system. In practice, these failures result in cluster-wide outage, a hung cluster, or a loss to all or a majority of the user data. Note that a bug resulting in under-replicated data blocks is not considered as catastrophic, even when it affect all data blocks, because it does not prevent users from their normal read and write to their data yet. We specifically study the catastrophic failures because they are the ones with the largest business impact to the vendors.

The fact that there are so many catastrophic failures is perhaps surprising given that the systems considered all have [**High Availability (HA) mechanisms**](#high-availability-mechanism) designed to prevent component failures from taking down the entire service. For example, all of the four systems with a chief and worker design — namely HBase, HDFS, MapReduce, and Redis — are designed to, on a chief node failure, automatically elect a new chief node and fail over to it. Cassandra is a peer-to-peer system, thus by design it avoids single points of failure. Then why do catastrophic failures still occur?

> Almost all catastrophic failures (92%) are the result of incorrect handling of non-fatal errors explicitly signaled in software.

![](/images/notes/fault-tolerance/catastrophic-failures.png)

These catastrophic failures are the result of more than one fault triggering, where the initial fault, whether due to a hardware fault, a misconfiguration, or a bug, first manifests itself explicitly as a non-fatal error — for example by throwing an exception or having a system call return an error. This error need not be catastrophic; however in the vast majority of cases, the handling of the explicit error was faulty, resulting in an error manifesting itself as a catastrophic failure.

This prevalence of incorrect error handling is unique to catastrophic failures. In comparison, only 25% of the non-catastrophic failures in our study involve incorrect error handling, indicating that in non-catastrophic failures, error handling was mostly effective in preventing the errors from taking down the entire service.

Overall, we found that the developers are good at anticipating possible errors. In all but one case, the errors were checked by the developers. The only case where developers did not check the error was an unchecked error system call return in Redis. This is different from the characteristics observed in previous studies on file system bugs, where many errors weren't even checked. This difference is likely because:

* the Java compiler forces developers to catch all the checked exceptions
* a variety of errors are expected to occur in large distributed systems, and the developers program more defensively

However, we found they were often simply sloppy in handling these errors. This is further corroborated in Findings 11 and 12 below. To be fair, we should point out that our findings are skewed in the sense that our study did not expose the many errors that are correctly caught and handled.

Nevertheless, the correctness of error handling code is particularly important given their impact. Previous studies show that the initial faults in distributed system failures are highly diversified (e.g., bugs, misconfigurations, node crashes, hardware faults), and in practice it is simply impossible to eliminate them all in large data centers. It is therefore unavoidable that some of these faults will manifest themselves into errors, and error handling then becomes the last line of defense.

Of the catastrophic failures we studied, only four were not triggered by incorrect error handling. Three of them were because the servers mistakenly threw fatal exceptions that terminated all the clients, i.e., the clients' error handling was correct. The other one was a massive performance degradation when a bug disabled DNS look-up result caching.

### Finding 11

> 35% of the catastrophic failures are caused by trivial mistakes in error handling logic — ones that simply violate best programming practices; and that can be detected without system specific knowledge.

Trivial mistakes can be broken into three categories:

* the error handler ignores explicit errors
* the error handler over-catches an exception and aborts the system
* the error handler contains "TODO" or "FIXME" in the comment

25% of the catastrophic failures were caused by ignoring explicit errors (an error handler that only logs the error is also considered as ignoring the error). For systems written in Java, the exceptions were all explicitly thrown, whereas in Redis they were system call error returns. Ignoring errors and allowing them to propagate is known to be bad programming practice, yet we observed this lead to many catastrophic failures. At least the developers were careful at logging the errors: all the errors were logged except for one case where the Redis developers did not log the error system call return.

Another 8% of the catastrophic failures were caused by developers prematurely aborting the entire cluster on a non-fatal exception. The safe practice is to catch the precise exception. An even more obvious mistake, where the developers only left a comment "TODO" in the handler logic in addition to a logging statement. While this error would only occur rarely, it took down a production cluster of 4,000 nodes.

### Finding 12

The other 57% of the catastrophic failures are caused by incorrect error handling where system-specific knowledge is required to detect the bugs.

> In 23% of the catastrophic failures, while the mistakes in error handling were system specific, they are still easily detectable. More formally, the incorrect error handling in these cases would be exposed by 100% statement coverage testing on the error handling logic.

In other words, once the problematic basic block in the error handling code is triggered, the failure is guaranteed to be exposed. This suggests that these basic blocks were completely faulty and simply never properly tested. Hence, a good strategy to prevent these failures is to start from existing error handling logic and try to reverse
engineer test cases that trigger them. For example, [symbolic execution techniques](https://en.wikipedia.org/wiki/Symbolic_execution) could be extended to purposefully reconstruct an execution path that can reach the error handling code block, instead of blindly exploring every execution path from the system entry points. While high statement coverage on error handling code might seem difficult to achieve, aiming for higher statement coverage in testing might still be a better strategy than a strategy of applying random fault injections.

Existing testing techniques for error handling logic primarily use a "top-down" approach: start the system using testing inputs or model-checking, and actively inject errors at different stages. Tools like LFI and Fate&Destini are intelligent to inject errors only at appropriate points and avoid duplicated injections. Such techniques inevitably have greatly improved the reliability of software systems. In fact, Hadoop developers have their own error injection framework to test their systems, and the production failures we studied are likely the ones missed by such tools.

However, our findings suggest that it could be challenging for such "top-down" approaches to further expose these remaining production failures. They require rare sequence of input events to first take the system to
a rare state, before the injected error can take down the service. In addition, 38% of the failures only occur in long-running systems. Therfore, the possible space of input events would simply be untractable.

The remaining 34% of catastrophic failures involve complex bugs in the error handling logic. These are the cases where developers did not anticipate certain error scenarios. These type of errors — which are almost byzantine — are indeed the hardest to test for. Detecting them require both understanding how the system works and anticipating all possible real-world failure modes. While our study cannot provide constructive suggestions on how to identify such bugs, we found they only account for one third of the catastrophic failures.

## Symptoms

### Unexpected termination

### Incorrect result

### Data loss or potential data loss

### Hung System

### Severe performance degradation

### Resource leak/exhaustion

## Mechanisms

Digital computer systems have special characteristics that determine how these systems fail and what fault tolerance mechanisms are appropriate:

* digital systems are discrete systems: Unlike continuous systems, such as analog control systems, they operate in discontinuous steps
* digital systems encode information: unlike continuous systems, values are represented by a series of encoded symbols * digital systems can modify their behavior based on the information they process

Since digital systems are discrete systems, results may be tested or compared before they are released to the outside world. While analog systems must continuously apply redundant or limiting values, a digital system may substitute an alternative result before sending an output value. While it is possible to build digital computers that operate asynchronously (without a master clock to sequence internal operations), in practice all digital computers are sequenced from a clock signal. This dependency on a clock makes an accurate clock source as important as a source of power, but it also means that identical sequences of instructions take essentially the same amount of time. One of the most common fault tolerance mechanisms, the *time-out*, uses this property to measure program activity (or lack of activity).

The fact that digital systems encode information is extremely important. The most important implication of information encoding is that digital systems can accurately store information for a long period of time, a capability not available in analog systems, which are subject to value drift. This also means that digital systems can store identical copies of information and expect the stored copies to still be identical after a substantial period of time.

Information encoding in digital systems may be redundant, with several codes representing the same value. Redundant encoding is the most powerful tool available to ensure that information in a digital system has not been changed during storage or transmission. Redundant encoding may be implemented at several levels in a digital system. At the lowest levels, carefully designed code patterns attached to blocks of digital information can allow special-purpose hardware to correct for a number of different communication or storage faults, including changes to single bits or changes to several adjacent bits.

### Redundancy Management

Fault tolerance is sometimes called redundancy management. For our purposes, **redundancy is the provision of functional capabilities that would be unnecessary in a fault-free environment**. Redundancy is necessary, but not sufficient for fault tolerance. For example, a computer system may provide redundant functions or outputs such that at least one result is correct in the presence of a fault, but if the user must somehow examine the results and select the correct one, then the only fault tolerance is being performed by the user. However, if the computer system correctly selects the correct redundant result for the user, then the computer system is not only redundant, but also fault tolerant. Redundancy management marshals the nonfaulty resources to provide the correct result.

Redundancy management or fault tolerance involves the following actions:

* **Fault Detection** - The process of determining that a fault has occurred
* **Fault Diagnosis** - The process of determining what caused the fault, or exactly which subsystem or component is faulty
* **Fault Containment** - The process that prevents the propagation of faults from their origin at one point in a system to a point where it can have an effect on the service to the user
* **Fault Masking** - The process of insuring that only correct values get passed to the system boundary in spite of a failed component.
* **Fault Compensation** - If a fault occurs and is confined to a subsystem, it may be necessary for the system to provide a response to compensate for output of the faulty subsystem.
* **Fault Repair** - The process in which faults are removed from a system. In welldesigned fault tolerant systems, faults are contained before they propagate to the extent that the delivery of system service is affected. This leaves a portion of the system unusable because of residual faults. If subsequent faults occur, the system may be unable to cope because of this loss of resources, unless these resources are reclaimed through a recovery process which insures that no faults remain in system resources or in the system state.

**The measure of success of redundancy management or fault tolerance is coverage**. Informally, coverage is the probability of a system failure given that a fault occurs. Simplistic estimates of coverage merely measure redundancy by accounting for the number of redundant success paths in a system. More sophisticated estimates of coverage account for the fact that each fault potentially alters a systems ability to resist further faults. The usual model is a [Markov process](https://en.wikipedia.org/wiki/Markov_model) in which each fault or repair action transitions the system into a new state, some of which are failure states. Because a distinct state is generated for each stage in each possible failure and repair process, Markov models for even simple systems can consist of thousands of states. Sophisticated analysis tools are available to analyze these models and to create the Markov models from more compact system descriptions such as [Petri Nets](https://en.wikipedia.org/wiki/Petri_net).

The implementation of the actions described above depend upon the form of redundancy employed such as **space redundancy** or **time redundancy**.

#### Space Redundancy

Space redundancy provides separate physical copies of a resource, function, or data item. Since it has been relatively easy to predict and detect faults in individual hardware units, such as processors, memories, and communications links, space redundancy is the approach most commonly associated with fault tolerance. It is effective when dealing with persistent faults, such as permanent component failures. Space redundancy is also the approach of choice when fault masking is required, since the redundant results are available simultaneously. The major concern in managing space redundancy is the elimination of failures caused by a fault to a function or resource that is common to all of the space-redundant units.

##### Time Redundancy

Digital systems have two unique advantages over other types of systems, including analog electrical systems. First, they can shift functions in time by storing information and programs for manipulating information. This means that if the expected faults are transient, a function can be rerun with a stored copy of the input data at a time sufficiently
removed from the first execution of the function that a transient fault would not affect both. Second, since digital systems encode information as symbols, they can include redundancy in the coding scheme for the symbols. This means that information shifted in time can be checked for unwanted changes, and in many cases, the information can be corrected to its original value.

![](/images/notes/fault-tolerance/time-and-space-redundancy.png)
<figcaption>The two sets of resources represent space redundancy and the sequential computations represent time redundancy. In the figure, time redundancy is not capable of tolerating the permanent fault in the top processing resource, but is adequate to tolerate the transient fault in the lower resource. In this simple example, there is still the problem of recognizing the correct output.</figcaption>

#### Clocks

Many fault tolerance mechanisms, employing either space redundancy or time redundancy, rely on an accurate source of time. Probably no hardware feature has a greater effect on fault tolerance mechanisms than a clock. An early decision in the development of a fault tolerant system should be the decision to provide a reliable time service throughout the system. Such a service can be used as a foundation for fault detection and repair protocols. If the time service is not fault tolerant, then additional interval timers must be added or complex asynchronous protocols must be implemented that rely on the progress of certain computations to provide an estimate of time. Multiple-processor system designers must decide to provide a fault tolerant global clock service that maintains a consistent source of time throughout the
system, or to resolve time conflicts on an ad-hoc basis.

#### Fault Containment Regions

Although it is possible to tailor fault containment policies to individual faults, it is common to divide a system into fault containment regions with few or no common dependencies between regions.

Fault containment regions attempt to prevent the propagation of data faults by limiting the amount of communication between regions to carefully monitored messages and the propagation of resource faults by eliminating shared resources. In some ultra-dependable designs, each fault containment region contains one or more physically and electrically isolated processors, memories, power supplies, clocks, and communication links. The only resources that are tightly coordinated in such architectures are clocks, and extensive precautions are taken to insure that clock synchronization mechanisms do not allow faults to propagate between regions. Data fault propagation is inhibited by locating redundant copies of critical programs in different fault containment regions and by accepting data from other copies only if multiple copies independently produce the same result.

#### Common Mode Failures

System failures occur when faults propagate to the outer boundary of the system. The goal of fault tolerance is to intercept the propagation of faults so that failure does not occur, usually by substituting redundant functions for functions affected by a particular fault. Occasionally, a fault may affect enough redundant functions that it is not possible to reliably select a non-faulty result, and the system will sustain a **common-mode failure**. A common-mode failure results from a single fault (or fault set). Computer systems are vulnerable to common-mode resource failures if they rely on a single source of power, cooling, or I/O. A more insidious source of common-mode failures is a design fault that causes redundant copies of the same software process to fail under identical conditions.

#### Encoding

Encoding is the primary weapon in the fault tolerance arsenal. Low-level encoding decisions are made by memory and processor designers when they select the error detection and correction mechanisms for memories and data buses. Communications protocols provide a variety of detection and correction options, including the encoding of large blocks of data to withstand multiple contiguous faults and provisions for multiple retries in case error correcting facilities cannot cope with faults. Long-haul communication facilities even provide for a negotiated fall-back in transmission speed to cope with noisy environments. These facilities should be supplemented with high-level encoding techniques that record critical system values using unique patterns that are unlikely to be randomly created.

### Acceptance Test Techniques

The **fault detection** mechanism used influences the remainder of the *other* fault tolerance activities (diagnosis, containment, masking, compensation, and recovery). The two common mechanisms for fault detection are acceptance tests and comparison.

#### Fault Detection

Acceptance tests are the more general fault detection mechanism in that they can be used even if the system is composed of a single (non-redundant) processor. The program or subprogram is executed and the result is subjected to a test. If the result passes the test, execution continues normally. A failed acceptance test is a symptom of a fault.

#### Fault Diagnosis

An acceptance test cannot generally be used to determine what has gone wrong. It can only tell that something has gone wrong.

#### Fault Containment

An acceptance test provides a barrier to the continued propagation of a fault. Further execution of the program being tested is not allowed until some form of retry successfully passes the acceptance test. If no alternatives pass the acceptance test, the subsystem fails, preferably silently. The silent failure of faulty components allows the rest of the system to continue in operation (where possible) without having to worry about erroneous output from the faulty component.

#### Fault Masking

An acceptance test successfully masks a bad value if a retry or alternate results in a new, correct result within the time limit set for declaring failure.

#### Fault Compensation

A program that fails an acceptance test can be replaced by an alternate. If the alternate passes the acceptance test, its result may be used to compensate for the original result. Notice that the alternate program run during a retry may be a very simple one that just outputs a "safe" value to compensate for the faulty subsystem. A common approach in control systems is to "coast" the result by providing the value calculated from the last known good cycle.

#### Fault Repair

Acceptance tests are usually used in a construct known as a recovery block. A recovery block provides backward fault recovery by rolling program execution back to the state before the faulty function was executed. This repairs the faulty state and the result. When a result fails an acceptance test, the program can be executed again before leaving the recovery block. If the new result passes the acceptance test, it can be assumed that the fault originally detected
was transient. If the software is suspect, an alternative can be executed in place of the original program fragment. If a single processor is used, the state of the processor must be reset to the beginning of the function in question. A mechanism called the *recovery cache* has been proposed to accomplish this. A recovery cache records the state of the processor at the entrance to each recovery block.

### Comparison Techniques

#### Fault Detection

Comparison is an alternative to acceptance tests for detecting faults. If the principal fault source is processor hardware, then multiple processors are used to execute the same program. As results are calculated, they are compared across processors. A mismatch indicates the presence of a fault. This comparison can be **pair-wise**, or it may involve three or more processors simultaneously. In the latter case the mechanism used is generally referred to as **voting**. If software design faults are a major consideration, then a comparison is made between the results from multiple versions of the software in question, a mechanism known as [n-version programming](https://en.wikipedia.org/wiki/N-version_programming).

#### Fault Diagnosis

Fault diagnosis with comparison is dependent upon whether pair-wise or voting comparison is used:

* **pair-wise** - when a mismatch occurs for a pair it is impossible to tell which of the processors has failed, the entire pair must be declared faulty
* **voting** - when three or more processors are running the same program, the processor whose values do not match the others is easily diagnosed as the faulty one

Voting may be **centralized** or **decentralized**.

Centralized voting is easy to mechanize, either in software or hardware, but results in a single point of failure, a violation of many qualitative requirements specifications. It is possible to compensate for total voter failure using a *chief and worker* approach that replaces a silent voter with a standby voter, as in the *pair and spare approach*.

Decentralized voting avoids the single point of failure, but requires a consensus among multiple voting agents, either hardware or software in order to avoid replication faults. In order to reach consensus, the distributed voters must synchronize to exchange several rounds of messages. In the worst case, where up to faulty processors are allowed to send misleading results to other processors participating in the consensus process, distributed voters must be provided to reach a state known as interactive consistency. Interactive consistency requires that each non-faulty processor provides a value, that all non-faulty processors agree on the same set of values, and that the values are correct for each of the non-faulty processors. Similar processes are required to maintain a consensus as to the number of members remaining in a group of distributed processors.

#### Fault Containment

When pair-wise comparison is used, containment is achieved by stopping all activity in the mismatching pair. Any other pairs in operation can continue executing the application, undisturbed. They detect the failure of the miscomparing pair through time-outs. When voting is used, containment is achieved by ignoring the failed processor and reconfiguring it out of the system.

#### Fault Masking

In a comparison based system, fault masking is achievable in two ways. When voting is used the voter only allows the correct value to pass on. If hardware voters are used, this usually occurs quickly enough to meet any response deadlines. If the voting is done by software voters that must reach a consensus, adequate time may not be available.

Pair-wise comparison requires the existence of multiple pairs of processors to mask faults. In this case the faulty pair of processors is halted, and values are obtained from the functional, good pairs.

#### Fault Compensation

The value provided by a voter may be the majority value, the median value, a plurality value, or some other predetermined satisfactory value. While this choice is application dependent, the most common choice is the median value. This guarantees that the value selected was calculated by at least one of the participating processors and that it is not an extreme value.

#### Fault Repair

In a comparison-based system with a single pair of processors, there is no recovery from a fault. With multiple pairs of pairs, recovery consists of using the values from the "good" pair. Some systems provide mechanisms to restart the miscomparing pair with data from a "good" pair. If the miscomparing pair subsequently produces results that compare for an adequate period of time, it may be configured back into the system.

When voting is used, recovery from a failed processor is accomplished by utilizing the "good" values from the other processors. A processor that is outvoted may be allowed to continue execution and may be configured back into the system if it successfully matches in a specified number of subsequent votes.

### Diversity

The only fault tolerance approach for combating common-mode design errors is design diversity — the implementation of more than one variant of the function to be performed. For computer-based applications, it is generally accepted that it is more effective to vary a design at higher levels of abstraction (i.e., by varying the algorithm or physical principles used to obtain a result) than to vary implementation details of a design (i.e. by using different programming languages or low level coding techniques). Since diverse designs must implement a common system specification, the possibility for dependencies always arises in the process of refining the specification to reflect difficulties uncovered in the implementation process. Truly diverse designs would eliminate dependencies on common design teams, design philosophies, software tools and languages, and even test philosophies. Many approaches attempt to achieve the necessary independence through randomness, by creating separate design teams that remain physically separate throughout the design, implementation, and test process. Recently, some projects have attempted to create diversity by enforcing differing design rules for the multiple teams.

## Rules

### Rule 1

A system is said to have a failure if the service it delivers to the user deviates from compliance with the system specification. A fault is the adjudged cause of a failure. The significance of this is that, in the absence of precise requirements, it is impossible to tell whether a system has failed, and therefore whether a fault has occurred.

Know precisely what the system is supposed to do. Part of this process should be determining how long a system can be allowed to deviate from its specification before the deviation is declared a failure.

### Rule 2

However, it is not sufficient to know what the system is supposed to do under normal circumstances. It is also necessary to know what abnormal conditions the system must accommodate. It is virtually impossible to enumerate the set of all possible faults that a system might encounter. It is much more manageable to deal with classes of faults.

Look at what can go wrong, and try to group the causes into classes for easier manageability. This involves defining a fault floor based on your ability to diagnose and repair faults.

### Rule 3

The goal of fault tolerance is to prevent faults from propagating to the system boundary, where it becomes observable and, hence, a failure. In general, the further a fault has propagated, the harder it is to deal with. Since fault tolerance is redundancy management, however, it becomes a matter of the degree of redundancy desired. For instance, it is almost certainly cheaper to deal with memory faults by using error correcting memory (that is, redundant bits in a memory location) than by providing a "shadow" memory. Note, however, that dealing with faults earlier rather than later may go counter to the advice given above regarding dealing with classes of faults rather than individual faults.

Study your application and determine appropriate fault containment regions and the earliest feasible time to deal with potential faults.

### Rule 4

In general, the price paid for a fault tolerant system is additional resources, both in terms of time, and in terms of space, (*and in terms of human resourses?*). As with most things these two can be traded off against each other. In some applications (e.g., flight control), timing is everything, even at the cost of extra processors. In general, the comparison approach to fault detection works best in these situations. In other applications (e.g., a space probe), weight and power consumption is an overriding issue—arguing for a higher reliance on time redundancy and suggesting the use of acceptance tests.

Completely understand the requirements of your application and use them to make appropriate time / space trade-offs.

### Rule 5

Protecting a system from every conceivable fault can exhaust another resource — money. This is true even if a rational set of fault classes is defined. The trade-off here is fault coverage versus the cost of that coverage. In all systems it is possible to classify faults by the likelihood of occurrence.

Whenever possible, concentrate on the credible faults and ignore those less likely to occur unless they can be dealt with at little or no additional cost.

### Rule 6

Time is an essential element in any digital computer system, even in systems that do not claim to be real-time. It is important to define the minimum period of time a system can fail to provide its defined service before a failure is declared. Unnecessarily short failure margins force the system designer to resort to expensive fault tolerance mechanisms, such as real-time fault masking.

Carefully determine application failure margins and use the information to balance the degree of fault tolerance needed with the cost of implementation.

## High availability mechanism

## Chaos Engineering

---

#### References

[1] Walter L. Heimerdinger, Charles B. Weinstock, "A Conceptual Framework for System Fault Tolerance";

[2] Ding Yuan, Yu Luo, Xin Zhuang, Guilherme Renna Rodrigues, Xu Zhao, Yongle Zhang, Pranay U. Jain, Michael Stumm, "Simple Testing Can Prevent Most Critical Failures: An Analysis of Production Failures in Distributed Data-Intensive Systems";
