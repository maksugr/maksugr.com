---
title: 'DBMS'
summary: 'Database management systems summary'
publishedAt: '2022-05-05'
---

## History

Relational [DBMSs](#definition) arrived on the scene as research prototypes in the 1970's, in the form of [System R](https://en.wikipedia.org/wiki/IBM_System_R) and INGRES. The main thrust of both prototypes was to surpass IMS in value to customers on the applications that IMS was used for, namely "business data processing". Hence, both systems were architected for online transaction processing ([OLTP](#oltp)) applications, and their commercial counterparts (i.e., [DB2](https://en.wikipedia.org/wiki/IBM_Db2) and INGRES, respectively) found acceptance in this arena in the 1980's. Other vendors (e.g., Sybase, Oracle, and Informix) followed the same basic DBMS model, which:

* stores relational tables row-by-row
* uses B-trees for indexing
* uses a [cost-based optimizer](#cost-based-optimizer)
* provides [ACID](#acid) transaction properties

Since the early 1980's, the major DBMS vendors have steadfastly stuck to a "one size fits all" strategy, whereby they maintain a single code line with all DBMS services. The reasons for this choice are straightforward ― the use of multiple code lines causes various practical problems, including:

* a cost problem, because maintenance costs increase at least linearly with the number of code lines
* a compatibility problem, because all applications have to run against every code line
* a sales problem, because salespeople get confused about which product to try to sell to a customer
* a marketing problem, because multiple code lines need to be positioned correctly in the marketplace.

To avoid these problems, all the major DBMS vendors have followed the adage "put all wood behind one arrowhead".

In the early 1990's, a new trend appeared: enterprises wanted to gather together data from multiple operational databases into a [data warehouse](#data-warehouse) for [business intelligence purposes](/notes/bi). A typical large enterprise has 50 or so operational systems, each with an online user community who expect fast response time. System administrators were (and still are) reluctant to allow businessintelligence users onto the same systems:

* fearing that the complex ad-hoc queries from these users will degrade response time for the online community
* business-intelligence users often want to see historical trends, as well as correlate data from multiple operational databases.

These features are very different from those required by online users.

For these reasons, essentially every enterprise created a large data warehouse, and periodically "scraped" the data from operational systems into it. Business-intelligence users could then run their complex ad-hoc queries against the data in the warehouse, without affecting the online users. Although most warehouse projects were dramatically over budget and ended up delivering only a subset of promised functionality, they still delivered a reasonable return on investment. In fact, it is widely acknowledged that historical warehouses of retail transactions pay for themselves within a year, primarily as a result of more informed stock rotation and buying decisions.

## Data warehouse

Data warehouses are very different from [OLTP](#oltp) systems. OLTP systems have been optimized for updates, as the main business activity is typically to sell a good or service. In contrast, the main activity in data warehouses is ad-hoc queries, which are often quite complex. Hence, periodic load of new data interspersed with ad-hoc query activity is what a typical warehouse experiences. (*data-storage vs analytical systems [2]*)

The standard wisdom in data warehouse schemas is to create a [fact table](#fact-table), containing the "**who**, **what**, **when**, **where**" about each operational transaction.

![](/images/notes/dbms/typical-retailer-star-schema.png)
<figcaption>The schema for a typical retailer</figcaption>

Note the central fact table, which holds an entry for each item that is scanned by a cashier in each store in its chain. In addition, the warehouse contains dimension tables, with information on each store, each customer, each product, and each time period. In effect, the fact table contains a foreign key for each of these dimensions, and a [star schema](#star-schema) is the natural result. Such star schemas are omnipresent in warehouse environments, but are virtually nonexistent in OLTP environments.

It is a well known homily that warehouse applications run much better using *bit-map indexes* while OLTP users prefer *B-tree indexes*. The reasons are straightforward: bit-map indexes are faster and more compact on warehouse workloads, while failing to work well in OLTP environments. As a result, many vendors support both B-tree indexes and bit-map indexes in their DBMS products.

In addition, materialized views are a useful optimization tactic in warehouse worlds, but never in OLTP worlds. In contrast, normal ("virtual") views find acceptance in OLTP environments.

To a first approximation, most vendors have a warehouse DBMS (bit-map indexes, materialized views, star schemas and optimizer tactics for star schema queries) and an OLTP DBMS (B-tree indexes and a standard cost-based optimizer), which are united by a common parser.

![](/images/notes/dbms/common-parser.png)

Although this configuration allows such a vendor to market his DBMS product as a single system, because of the single user interface, in effect, she is selling multiple systems.

The illusion of "one size fits all" can be preserved as a marketing fiction for the two different systems because of the common user interface. In the stream processing market, to which we now turn, such a common front end is impractical. Hence, not only will there be different engines but also different front ends. The marketing fiction of "one size fits all" will not fly in this world.

### Row vs Column

All major DBMS vendors implement record-oriented storage systems, where the attributes of a record are placed contiguously in storage. Using this "**row-store**" architecture, a single disk write is all that is required to push all of the attributes of a single record out to disk. Hence, such a system is "**write-optimized**" because high performance on record writes is easily achievable. It is easy to see that write-optimized systems are especially effective on OLTP-style applications, the primary reason why most commercial DBMSs employ this architecture.

In contrast, warehouse systems need to be "**readoptimized**" as most workload consists of ad-hoc queries that touch large amounts of historical data. In such systems, a "**column-store**" model where the values for all of the rows of a single attribute are stored contiguously is drastically more efficient.

With a column-store architecture, a DBMS need only read the attributes required for processing a given query, and can avoid bringing into memory any other irrelevant attributes. Given that records with hundreds of attributes (with many null values) are becoming increasingly common, this approach results in a sizeable performance advantage for warehouse workloads where typical queries involve aggregates that are computed on a small number of attributes over large data sets.

> HBase and Cassandra are the top two most popular wide column store system [2].

### High Availability

It is a requirement of many stream-based applications to have high availability (HA) and stay up 7x24. Standard
DBMS logging and crash recovery mechanisms are ill-suited for the streaming world as they introduce several key problems.

First, log-based recovery may take large number of seconds to small numbers of minutes. During this period, the application would be "down". Such behavior is clearly undesirable in many real-time streaming domains (e.g., financial services). Second, in case of a crash, some effort must be made to buffer the incoming data streams, as otherwise this data will be irretrievably lost during the recovery process. Third, DBMS recovery will only deal with tabular state and will thus ignore operator states. For example, in the feed alarm application, the counters are not in stored in tables; therefore their state would be lost in a crash. One straightforward fix would be to force all operator state into tables to use DBMS-style recovery; however, this solution would significantly slow down the application.

The obvious alternative to achieve high availability is to use techniques that rely on [Tandem-style process pairs](https://en.wikipedia.org/wiki/Tandem_Computers). The basic idea is that, in the case of a crash, the application performs failover to a backup machine, which typically operates as a "hot standby", and keeps going with small delay. This approach eliminates the overhead of logging.

Unlike traditional data-processing applications that require precise recovery for correctness, many streamprocessing applications can tolerate and benefit from weaker notions of recovery. In other words, failover does not always need to be “perfect”. Consider monitoring applications that operate on data streams whose values are periodically refreshed. Such applications can often tolerate tuple losses when a failure occurs, as long as such interruptions are short. Similarly, if one loses a couple of ticks in the feed alarm application during failover, the correctness would probably still be preserved. In contrast, applications that trigger alerts when certain combinations of events happen, require that no tuples be lost, but may tolerate temporary duplication. For example, a patient monitoring application may be able to tolerate duplicate tuples ("heart rate is 79") but not lost tuples ("heart rate has changed to zero"). Of course, there will always be a class of applications that require strong, precise recovery guarantees. A financial application that performs portfolio management based on individual stock transactions falls into this category.

As a result, there is an opportunity to devise simplified and low overhead failover schemes, when weaker correctness notions are sufficient.

### Synchronization

Many stream-based applications rely on shared data and computation. Shared data is typically contained in a table that one query updates and another one reads. For example, the Linear Road application requires that vehicle-position data be used to update statistics on highway usage, which in turn are read to determine tolls for each segment on the highway. Thus, there is a basic need to provide isolation between messages.

Traditional DBMSs use ACID transactions to provide isolation (among others things) between concurrent transactions submitted by multiple users. In streaming systems, which are not multi-user, such isolation can be effectively achieved through simple critical sections, which can be implemented through light-weight semaphores. Since full-fledged transactions are not required, there is no need to use heavy-weight lockingbased mechanisms anymore. In summary, ACID properties are not required in most stream processing applications, and simpler, specialized performance constructs can be used to advantage.

## Definition

## Fact table

## Star schema

## ACID

## OLTP

## OLAP

## Cost-based optimizer

---

#### References

[1] Michael Stonebraker, Uğur Çetintemel, "'One Size Fits All': An Idea Whose Time Has Come and Gone";

[2] Ding Yuan, Yu Luo, Xin Zhuang, Guilherme Renna Rodrigues, Xu Zhao, Yongle Zhang, Pranay U. Jain, Michael Stumm, "Simple Testing Can Prevent Most Critical Failures: An Analysis of Production Failures in Distributed Data-Intensive Systems";
