---
title: 'Fault tolerance'
summary: 'Fault tolerance summary'
topics: ['fault tolerance']
publishedAt: '2022-04-25'
---

## Definitions

The terms *failure* and *fault* are key to any understanding of system reliability. Yet they are often misused.

Over time, failure has come to be defined in terms of specified service delivered by a system. This avoids circular definitions involving essentially synonymous terms such as *defect*. A system to have a **failure** if the service it delivers to the user deviates from compliance with the system specification for a specified period of time. While it may be difficult to arrive at an unambiguous specification of the service to be delivered by any system, the concept of an agreed-to specification is the most reasonable of the options for defining satisfactory service and the absence of satisfactory service, failure.

It is important to have some reference for the definition of failure, and the specification is a logical choice. The specification can be considered as a boundary to the system's [region of concern](#failure-regions). It is important to recognize that every system has an *explicit specification*, which is written, and an *implicit specification* that the system should at least behave as well as a reasonable person could expect based on experience with similar systems and with the world in general. Clearly, it is important to make as much of the specification as explicit as possible.

It has become the practice to define faults in terms of failures. The concept closest to the common understanding of the word fault is one that defines a **fault** as the adjudged cause of a failure. This fits with a common application of the verb form of the word fault, which involves determining cause or affixing blame. However, this requires an understanding of how failures are caused.

An alternate view of faults is to consider them failures in other systems that interact with the system under consideration:

* a subsystem internal to the system under consideration
* a component of the system under consideration
* an external system that interacts with the system under consideration
* the environment

In the first instance, the link between faults and failures is cause; in the second case it is level of abstraction or location.

The advantages of defining faults as failures of component/interacting systems are:

* one can consider faults without the need to establish a direct connection with a failure, so we can discuss faults that do not cause failures, i.e., the system is naturally fault tolerant
* the definition of a fault is the same as the definition of a failure with only the boundary of the relevant system or subsystem being different

This means that we can consider an obvious internal defect to be a fault without having to establish a causal relationship between the defect and a failure at the system boundary.

A fault can lead to:

* other faults
* failure
* nothing

A system with faults may continue to provide its service, that is, not fail. Such a system is said to be **fault tolerant**. Thus, an important motivation for differentiating between faults and failures is the need to describe the fault tolerance of a system. An observer inspecting the internals of the system would say that the faulty component had failed, because the observer's viewpoint is now at a lower level of detail.

The observable effect of a fault at the system boundary is called a **symptom**. The most extreme symptom of a fault is a failure.

Note that a fault does not lead to a failure unless the result is observable by the user. This means that one person's fault is another person's failure.

The term **error** often is used in addition to the terms fault and failure. Often, errors are defined to be the result of faults, leading to failures. Informally, errors seem to be a passive concept associated with incorrect values in the system state. However, it is extremely difficult to develop unambiguous criteria for differentiating between faults and errors. Many researchers refer to value faults, which are also clearly erroneous values. The connection between error and failure is even more difficult to describe.

As we have seen, differentiation between failures and faults is essential for fault tolerant systems. A third term error, adds little to this distinction and can be a source of confusion.

## Requirements

To be fault tolerant, [a system](/notes/system) must be able to:

* detect
* diagnose
* confine
* mask
* compensate
* recover

## Degree

The degree of fault tolerance a system requires can be specified [quantitatively](#quantitative-goals) or [qualitatively](#qualitative-goals).

### Quantitative Goals

A quantitative reliability goal is usually expressed as the maximum allowed failure-rate. For example, the reliability figure usually stated as a goal for computer systems in commercial aircraft is less than 10<sup>-9</sup> failures per hour. The problem with stating reliability requirements in this manner is that it is difficult to know when it has been achieved. Standard statistical methods cannot be used to show such reliability with either standard or fault tolerant software. It is also clear that there is no way to achieve confidence that a system meets such a reliability goal through random testing. Nevertheless, reliability goals are often expressed in this manner.

### Qualitative Goals

An alternative method of specifying a system's reliability characteristics is to specify them qualitatively. Typical specifications would include next goals.

#### Fail-safe

Design the system so that, when it sustains a specified number of faults, it fails in a safe mode. For instance, railway signalling systems are designed to fail so that all trains stop.

#### Fail-op

Design the system so that, when it sustains a specified number of faults, it still provides a subset of its specified behavior.

#### No single point of failure

Design the system so that the failure of any single component will not cause the system to fail. Such systems are often designed so that the failed component can be replaced or repaired before another failure occurs.

#### Consistency

Design the system so that all information delivered by the system is equivalent to the information that would be delivered by an instance of a non-faulty system.

## Levels

There are three levels at which fault tolerance can be applied:

- [Hardware](#hardware-fault-tolerance)
- [Software](#software-fault-tolerance)
- [System](#system-fault-tolerance)

### Hardware fault tolerance

Traditionally, fault tolerance has been used to compensate for faults in computing resources. By managing extra hardware resources, the computer subsystem increases its ability to continue operation. Hardware fault tolerance measures include:

* redundant communications
* replicated processors
* additional memory
* redundant power supplies.

Hardware fault tolerance was particularly important in the early days of computing when the time between machine failures was measured in minutes.

### Software fault tolerance

The second level of fault tolerance recognizes that a fault-tolerant hardware platform does not, in itself, guarantee high availability to the system user. It is still important to structure the computer software to compensate for faults such as changes in programs or data structures due to transients or design errors. This is software fault tolerance. Mechanisms such as:

* [checkpoint](#checkpoint)
* [restart](#restart)
* [recovery blocks](#recovery-blocks)
* [multiple-version programs](#multiple-version-programs)

are often used at this level.

#### Checkpoint

#### Restart

#### Recovery blocks

#### Multiple-version programs

### System fault tolerance

At a third level, the computer subsystem may provide functions that compensate for failures in other system facilities that are not computer-based. This is system fault tolerance. For example, software can detect and compensate for failures in sensors. Measures at this level are usually application-specific. It is important that fault tolerance measures at all levels be compatible.

## Dependency relations

### Acyclic / Cyclic

A component of a system is said to **depend on another** component if the correctness of the first component's behavior requires the correct operation of the second component. Traditionally, the set of possible dependencies in a system are considered to form an acyclic graph. The term **fault tree** analysis seems to imply this, among other things. Indeed many systems exhibit this behavior, in which one fault leads to another which leads to another until eventually a failure occurs. It is possible, however, for a dependency relationship to cycle back upon itself. A dependency relationship is said to be **acyclic** if it forms part of a tree. A **cyclic** dependency relationship is one that cannot be described as part of a tree, but rather must be described as part of a directed cyclic graph.

### Failure regions

Defining a **failure region** limits the consideration of faults and failures to a portion of a system and its environment. This is necessary to insure that system specification, analysis and design efforts are concentrated on the portions of a system that can be observed and controlled by the designer and user. It helps to simplify an otherwise overwhelming task.

A system is typically made up of lots of components parts. These components are, in turn, made up of sub-components This continues arbitrarily until an **atomic component** (a component that is not divisible or that we choose not to divide into sub-components) is reached. Although all components are theoretically capable of having faults, for any system there is a level beyond which the faults are "not interesting". This level is called the **fault floor**. Atomic
components lie at the fault floor. **We are concerned with faults emerging from atomic components, but not faults that lie within these components**.

Similarly, as components are aggregated into a system, eventually the system is complete. Everything else (the user, the environment) is not a part of the system. This is the **system boundary**. **Failures occur when faults reach the system boundary**.

The **span of concern** begins at the boundaries between the system and the user and between the system and the environment, and ends at the fault floor. Faults below the fault floor are indistinguishable, either because they are not fully understood, or because they are too numerous. Informally, **the span of concern is the area within which faults are of interest**.

![](/images/fault-tolerance/span-of-concern.png)

## Fault classes

No system can be made to tolerate all possible faults, so it is essential that the faults be considered throughout the requirements definition and system design process. However, it is impractical to enumerate all of the faults to be tolerated; faults must be aggregated into manageable **fault classes**.

Faults may be classified based on:

* [Locality](#locality)
* [Effect](#effect)
* [Cause](#immediate-cause)
* [Duration](#duration)
* Effect on System State

### Locality

#### Atomic Component Faults

A atomic component fault is a fault at the fault floor, that is, in a component that cannot be subdivided for analysis purposes.

#### Composite Component Faults

A composite component fault is one that arises within an aggregation of atomic components rather than in an atomic component. It may be the result of one or more atomic component faults.

#### System Level Faults

A system level fault is one that arises in the structure of a system rather than in the system's components. Such faults are usually interaction or integration faults, that is, they occur because of the way the system is assembled rather than because of the integrity of any individual component. Note that an inconsistency in the operating rules for a system may lead to a system level fault. System level faults also include operator faults, in which an operator does not correctly perform his or her role in system operation.

Systems that distribute objects or information are prone to a special kind of system fault: **replication faults**. Replication faults occur when replicated information in a system becomes inconsistent, either because replicates that
are supposed to provide identical results no longer do so, or because the aggregate of the data from the various replicates is no longer consistent with system specifications.

Replication faults can be caused by **malicious faults**, in which components such as processors "lie" by providing conflicting versions of the same information to other components in the system. Malicious faults are sometimes called **[Byzantine faults](https://en.wikipedia.org/wiki/Byzantine_fault)** after an early formulation of the problem in terms of Byzantine generals trying to reach a consensus on attacking when one of the generals is a traitor.

#### External Faults

External faults arise from outside the system boundary, the environment, or the user. **Environmental faults** include phenomena that directly affect the operation of the system, such as temperature, vibration, or nuclear or electromagnetic radiation or that affect the inputs provided to the system. **User faults** are created by the user in employing the system. Note that the roles of user and operator are considered separately; the user is considered to be external to the system while the operator is considered to be a part of the system.

### Effect

Faults may also be classified according to their effect on the user of the system or service. Since computer system components interact by exchanging data values in a specified time and/or sequence, fault effects can be cleanly separated into timing faults and value faults. Timing faults occur when a value is delivered before or after the specified time. Value faults occur when the data differs in value from the specification.

#### Value Faults

Computer systems communicate by providing values. A value fault occurs when a computation returns a result that does not meet the system's specification. Value faults are usually detected using knowledge of the allowable values of the data, possibly determined at run time.

#### Timing Faults

A timing fault occurs when a process or service is not delivered or completed within the specified time interval. Timing faults cannot occur if there is no explicit or implicit specification of a deadline. Timing faults can be detected by observing the time at which a required interaction takes place; no knowledge of the data involved is usually needed.

Since time increases monotonically, it is possible to further classify timing faults into:

* early
* late
* "never" (omission)

Since it is practically impossible to determine if "never" occurs, omission faults are really late timing faults that exceed an arbitrary limit. Systems that never produce value faults, but only fail by omission are called **fail-silent systems**. If all failures require system restart, the system is a **fail-stop system**.

### Duration

#### Persistent faults

Persistent faults remain active for a significant period of time. These faults are sometimes termed hard faults. Persistent faults usually are the easiest to detect and diagnose, but may be difficult to contain and mask unless redundant hardware is available. Persistent faults can be effectively detected by test routines that are interleaved with normal processing.

#### Transient faults

Transient faults remain active for a short period of time. Because of their short duration, transient faults are often detected through the faults that result from their propagation.

#### Periodic faults

A transient fault that becomes active periodically is a periodic fault (sometimes referred to as an *intermittent fault*).

### Immediate (*direct*) Cause

Faults can be classified according to the operational condition that causes them.

#### Resource depletion faults

Resource depletion faults occur when a portion of the system is unable to obtain the resources required to perform its task. Resources may include time on a processing or communications device, storage, power, logical structures such as a data structure, or a physical item such as a processor.

#### Logic faults

Logic faults occur when adequate resources are available, but the system does not behave according to specification. Logic faults may be the result of improper design or implementation. Logic faults may occur in hardware or software.

#### Physical faults

Physical faults occur when hardware breaks or a mutation occurs in executable software. Most common fault tolerance mechanisms deal with hardware faults.

### Ultimate (*original*) Cause

Faults can also be classified as to their ultimate cause. Ultimate causes are the things that must be fixed to eliminate a fault. These faults occur during the development process and are most effectively dealt with using [fault avoidance](/notes/dependable-system#fault-avoidance) and [fault removal](/notes/dependable-system#fault-removal) techniques.

#### Specification fault

A common ultimate cause of a fault is an improper requirements specification which leads to a specification fault. Technically this is not a fault, since a fault is defined to be the failure of a component/interacting systems and a failure is the deviation of the system from specification. However, it can be the reason a system deviates from the behavior expected by the user. An especially insidious instance of this arises when the requirements ignore aspects of the environment in which the system operates. For instance, radiation causing a bit to flip in a memory location would be a *value fault* which would be considered an *external fault*. However, if the fault propagates inside the system boundary the ultimate cause is a *specification fault* because the system specification did not foresee the problem.

#### Design fault

Flowing down the waterfall, a design fault results when the system design does not correctly match the requirements. The validation process is specifically designed to detect these faults.

#### Implementation fault

Flowing down the waterfall, an implementation fault arises when the system implementation does not adequately implement the design. The validation process is specifically designed to detect these faults.

#### Documentation fault

A documentation fault occurs when the documented system does not match the real system.

### Effect on System State

#### Crash

#### Amnesia

#### Partial amnesia

## Fault Attributes

### Observability

Faults originate in a system component or subsystem, in the system's environment, or in an interaction between the system and a user, operator, or another subsystem. A fault may ultimately have one of several effects:

* It may disappear with no perceptible effect
* It may remain in place with no perceptible effect
* It may lead to a sequence of additional faults that result in a failure in the system's delivered service (propagation to failure)
* It may lead to a sequence of additional faults with no perceptible effect on the system (undetected propagation)
* It may lead to a sequence of additional faults that have a perceptible effect on the system but do not result in a failure in the system's delivered service (detected propagation without failure)

Fault detection is usually the first step in fault tolerance. Even if other elements of a system prevent a failure by compensating for a fault, it is important to detect and remove faults to avoid the exhaustion of a systems fault tolerance resources.

A fault is **observable** if there is information about its existence available at the system interface. The information that indicates the existence of a fault is a **symptom**. A symptom may be a directly observed fault or failure, or it may be a change in system behavior such that the system still meets its specifications. A fault that a fault tolerance mechanism of a system has found is said to be **detected**. Otherwise it is **latent**, whether it is observable or not. The definition of detected is independent of whether or not the fault tolerance mechanism is able to successfully deal with the fault condition. For a fault to be detected, it is sufficient that it be known about.

### Propagation

A fault that propagates to other faults or failures is said to be **active**. A non-propagating fault is said to be **dormant**. When a previously dormant fault becomes active it is said to be **triggered**. An active fault may again become dormant, awaiting a new trigger. The sequence of faults, each successive one triggered by the preceding one and possibly ending in a failure, is known as a **fault trajectory**. Because of the ways faults trigger successive faults, a fault trajectory could be viewed as a chain reaction.

![](/images/fault-tolerance/fault-attributes.png)

## Mechanisms

Digital computer systems have special characteristics that determine how these systems fail and what fault tolerance mechanisms are appropriate:

* digital systems are discrete systems: Unlike continuous systems, such as analog control systems, they operate in discontinuous steps
* digital systems encode information: unlike continuous systems, values are represented by a series of encoded symbols * digital systems can modify their behavior based on the information they process

Since digital systems are discrete systems, results may be tested or compared before they are released to the outside world. While analog systems must continuously apply redundant or limiting values, a digital system may substitute an alternative result before sending an output value. While it is possible to build digital computers that operate asynchronously (without a master clock to sequence internal operations), in practice all digital computers are sequenced from a clock signal. This dependency on a clock makes an accurate clock source as important as a source of power, but it also means that identical sequences of instructions take essentially the same amount of time. One of the most common fault tolerance mechanisms, the *time-out*, uses this property to measure program activity (or lack of activity).

The fact that digital systems encode information is extremely important. The most important implication of information encoding is that digital systems can accurately store information for a long period of time, a capability not available in analog systems, which are subject to value drift. This also means that digital systems can store identical copies of information and expect the stored copies to still be identical after a substantial period of time.

Information encoding in digital systems may be redundant, with several codes representing the same value. Redundant encoding is the most powerful tool available to ensure that information in a digital system has not been changed during storage or transmission. Redundant encoding may be implemented at several levels in a digital system. At the lowest levels, carefully designed code patterns attached to blocks of digital information can allow special-purpose hardware to correct for a number of different communication or storage faults, including changes to single bits or changes to several adjacent bits.

### Redundancy Management

Fault tolerance is sometimes called redundancy management. For our purposes, **redundancy is the provision of functional capabilities that would be unnecessary in a fault-free environment**. Redundancy is necessary, but not sufficient for fault tolerance. For example, a computer system may provide redundant functions or outputs such that at least one result is correct in the presence of a fault, but if the user must somehow examine the results and select the correct one, then the only fault tolerance is being performed by the user. However, if the computer system correctly selects the correct redundant result for the user, then the computer system is not only redundant, but also fault tolerant. Redundancy management marshals the nonfaulty resources to provide the correct result.

Redundancy management or fault tolerance involves the following actions:

* **Fault Detection** - The process of determining that a fault has occurred
* **Fault Diagnosis** - The process of determining what caused the fault, or exactly which subsystem or component is faulty
* **Fault Containment** - The process that prevents the propagation of faults from their origin at one point in a system to a point where it can have an effect on the service to the user
* **Fault Masking** - The process of insuring that only correct values get passed to the system boundary in spite of a failed component.
* **Fault Compensation** - If a fault occurs and is confined to a subsystem, it may be necessary for the system to provide a response to compensate for output of the faulty subsystem.
* **Fault Repair** - The process in which faults are removed from a system. In welldesigned fault tolerant systems, faults are contained before they propagate to the extent that the delivery of system service is affected. This leaves a portion of the system unusable because of residual faults. If subsequent faults occur, the system may be unable to cope because of this loss of resources, unless these resources are reclaimed through a recovery process which insures that no faults remain in system resources or in the system state.

**The measure of success of redundancy management or fault tolerance is coverage**. Informally, coverage is the probability of a system failure given that a fault occurs. Simplistic estimates of coverage merely measure redundancy by accounting for the number of redundant success paths in a system. More sophisticated estimates of coverage account for the fact that each fault potentially alters a systems ability to resist further faults. The usual model is a [Markov process](https://en.wikipedia.org/wiki/Markov_model) in which each fault or repair action transitions the system into a new state, some of which are failure states. Because a distinct state is generated for each stage in each possible failure and repair process, Markov models for even simple systems can consist of thousands of states. Sophisticated analysis tools are available to analyze these models and to create the Markov models from more compact system descriptions such as [Petri Nets](https://en.wikipedia.org/wiki/Petri_net).

The implementation of the actions described above depend upon the form of redundancy employed such as **space redundancy** or **time redundancy**.

#### Space Redundancy

Space redundancy provides separate physical copies of a resource, function, or data item. Since it has been relatively easy to predict and detect faults in individual hardware units, such as processors, memories, and communications links, space redundancy is the approach most commonly associated with fault tolerance. It is effective when dealing with persistent faults, such as permanent component failures. Space redundancy is also the approach of choice when fault masking is required, since the redundant results are available simultaneously. The major concern in managing space redundancy is the elimination of failures caused by a fault to a function or resource that is common to all of the space-redundant units.

##### Time Redundancy

Digital systems have two unique advantages over other types of systems, including analog electrical systems. First, they can shift functions in time by storing information and programs for manipulating information. This means that if the expected faults are transient, a function can be rerun with a stored copy of the input data at a time sufficiently
removed from the first execution of the function that a transient fault would not affect both. Second, since digital systems encode information as symbols, they can include redundancy in the coding scheme for the symbols. This means that information shifted in time can be checked for unwanted changes, and in many cases, the information can be corrected to its original value.

![](/images/fault-tolerance/time-and-space-redundancy.png)
<figcaption>The two sets of resources represent space redundancy and the sequential computations represent time redundancy. In the figure, time redundancy is not capable of tolerating the permanent fault in the top processing resource, but is adequate to tolerate the transient fault in the lower resource. In this simple example, there is still the problem of recognizing the correct output.</figcaption>

#### Clocks

Many fault tolerance mechanisms, employing either space redundancy or time redundancy, rely on an accurate source of time. Probably no hardware feature has a greater effect on fault tolerance mechanisms than a clock. An early decision in the development of a fault tolerant system should be the decision to provide a reliable time service throughout the system. Such a service can be used as a foundation for fault detection and repair protocols. If the time service is not fault tolerant, then additional interval timers must be added or complex asynchronous protocols must be implemented that rely on the progress of certain computations to provide an estimate of time. Multiple-processor system designers must decide to provide a fault tolerant global clock service that maintains a consistent source of time throughout the
system, or to resolve time conflicts on an ad-hoc basis.

#### Fault Containment Regions

Although it is possible to tailor fault containment policies to individual faults, it is common to divide a system into fault containment regions with few or no common dependencies between regions.

Fault containment regions attempt to prevent the propagation of data faults by limiting the amount of communication between regions to carefully monitored messages and the propagation of resource faults by eliminating shared resources. In some ultra-dependable designs, each fault containment region contains one or more physically and electrically isolated processors, memories, power supplies, clocks, and communication links. The only resources that are tightly coordinated in such architectures are clocks, and extensive precautions are taken to insure that clock synchronization mechanisms do not allow faults to propagate between regions. Data fault propagation is inhibited by locating redundant copies of critical programs in different fault containment regions and by accepting data from other copies only if multiple copies independently produce the same result.

#### Common Mode Failures

System failures occur when faults propagate to the outer boundary of the system. The goal of fault tolerance is to intercept the propagation of faults so that failure does not occur, usually by substituting redundant functions for functions affected by a particular fault. Occasionally, a fault may affect enough redundant functions that it is not possible to reliably select a non-faulty result, and the system will sustain a **common-mode failure**. A common-mode failure results from a single fault (or fault set). Computer systems are vulnerable to common-mode resource failures if they rely on a single source of power, cooling, or I/O. A more insidious source of common-mode failures is a design fault that causes redundant copies of the same software process to fail under identical conditions.

#### Encoding

Encoding is the primary weapon in the fault tolerance arsenal. Low-level encoding decisions are made by memory and processor designers when they select the error detection and correction mechanisms for memories and data buses. Communications protocols provide a variety of detection and correction options, including the encoding of large blocks of data to withstand multiple contiguous faults and provisions for multiple retries in case error correcting facilities cannot cope with faults. Long-haul communication facilities even provide for a negotiated fall-back in transmission speed to cope with noisy environments. These facilities should be supplemented with high-level encoding techniques that record critical system values using unique patterns that are unlikely to be randomly created.

---

#### References

* Walter L. Heimerdinger, Charles B. Weinstock, "A Conceptual Framework for System Fault Tolerance";
