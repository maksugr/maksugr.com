---
title: 'Fault tolerance'
summary: 'Fault tolerance summary'
topics: ['fault tolerance']
publishedAt: '2022-04-25'
---

## Definitions

The terms *failure* and *fault* are key to any understanding of system reliability. Yet they are often misused.

Over time, failure has come to be defined in terms of specified service delivered by a system. This avoids circular definitions involving essentially synonymous terms such as *defect*. A system to have a **failure** if the service it delivers to the user deviates from compliance with the system specification for a specified period of time. While it may be difficult to arrive at an unambiguous specification of the service to be delivered by any system, the concept of an agreed-to specification is the most reasonable of the options for defining satisfactory service and the absence of satisfactory service, failure.

It is important to have some reference for the definition of failure, and the specification is a logical choice. The specification can be considered as a boundary to the system's [region of concern](#failure-regions). It is important to recognize that every system has an *explicit specification*, which is written, and an *implicit specification* that the system should at least behave as well as a reasonable person could expect based on experience with similar systems and with the world in general. Clearly, it is important to make as much of the specification as explicit as possible.

It has become the practice to define faults in terms of failures. The concept closest to the common understanding of the word fault is one that defines a **fault** as the adjudged cause of a failure. This fits with a common application of the verb form of the word fault, which involves determining cause or affixing blame. However, this requires an understanding of how failures are caused.

An alternate view of faults is to consider them failures in other systems that interact with the system under consideration:

* a subsystem internal to the system under consideration
* a component of the system under consideration
* an external system that interacts with the system under consideration
* the environment

In the first instance, the link between faults and failures is cause; in the second case it is level of abstraction or location.

The advantages of defining faults as failures of component/interacting systems are:

* one can consider faults without the need to establish a direct connection with a failure, so we can discuss faults that do not cause failures, i.e., the system is naturally fault tolerant
* the definition of a fault is the same as the definition of a failure with only the boundary of the relevant system or subsystem being different

This means that we can consider an obvious internal defect to be a fault without having to establish a causal relationship between the defect and a failure at the system boundary.

A fault can lead to:

* other faults
* failure
* nothing

A system with faults may continue to provide its service, that is, not fail. Such a system is said to be **fault tolerant**. Thus, an important motivation for differentiating between faults and failures is the need to describe the fault tolerance of a system. An observer inspecting the internals of the system would say that the faulty component had failed, because the observer's viewpoint is now at a lower level of detail.

The observable effect of a fault at the system boundary is called a **symptom**. The most extreme symptom of a fault is a failure.

Note that a fault does not lead to a failure unless the result is observable by the user. This means that one person's fault is another person's failure.

The term **error** often is used in addition to the terms fault and failure. Often, errors are defined to be the result of faults, leading to failures. Informally, errors seem to be a passive concept associated with incorrect values in the system state. However, it is extremely difficult to develop unambiguous criteria for differentiating between faults and errors. Many researchers refer to value faults, which are also clearly erroneous values. The connection between error and failure is even more difficult to describe.

As we have seen, differentiation between failures and faults is essential for fault tolerant systems. A third term error, adds little to this distinction and can be a source of confusion.

## Requirements

To be fault tolerant, [a system](/notes/system) must be able to:

* detect
* diagnose
* confine
* mask
* compensate
* recover

## Degree

The degree of fault tolerance a system requires can be specified [quantitatively](#quantitative-goals) or [qualitatively](#qualitative-goals).

### Quantitative Goals

A quantitative reliability goal is usually expressed as the maximum allowed failure-rate. For example, the reliability figure usually stated as a goal for computer systems in commercial aircraft is less than 10<sup>-9</sup> failures per hour. The problem with stating reliability requirements in this manner is that it is difficult to know when it has been achieved. Standard statistical methods cannot be used to show such reliability with either standard or fault tolerant software. It is also clear that there is no way to achieve confidence that a system meets such a reliability goal through random testing. Nevertheless, reliability goals are often expressed in this manner.

### Qualitative Goals

An alternative method of specifying a system's reliability characteristics is to specify them qualitatively. Typical specifications would include next goals.

#### Fail-safe

Design the system so that, when it sustains a specified number of faults, it fails in a safe mode. For instance, railway signalling systems are designed to fail so that all trains stop.

#### Fail-op

Design the system so that, when it sustains a specified number of faults, it still provides a subset of its specified behavior.

#### No single point of failure

Design the system so that the failure of any single component will not cause the system to fail. Such systems are often designed so that the failed component can be replaced or repaired before another failure occurs.

#### Consistency

Design the system so that all information delivered by the system is equivalent to the information that would be delivered by an instance of a non-faulty system.

## Levels

There are three levels at which fault tolerance can be applied:

- [Hardware](#hardware-fault-tolerance)
- [Software](#software-fault-tolerance)
- [System](#system-fault-tolerance)

### Hardware fault tolerance

Traditionally, fault tolerance has been used to compensate for faults in computing resources. By managing extra hardware resources, the computer subsystem increases its ability to continue operation. Hardware fault tolerance measures include:

* redundant communications
* replicated processors
* additional memory
* redundant power supplies.

Hardware fault tolerance was particularly important in the early days of computing when the time between machine failures was measured in minutes.

### Software fault tolerance

The second level of fault tolerance recognizes that a fault-tolerant hardware platform does not, in itself, guarantee high availability to the system user. It is still important to structure the computer software to compensate for faults such as changes in programs or data structures due to transients or design errors. This is software fault tolerance. Mechanisms such as:

* [checkpoint](#checkpoint)
* [restart](#restart)
* [recovery blocks](#recovery-blocks)
* [multiple-version programs](#multiple-version-programs)

are often used at this level.

#### Checkpoint

#### Restart

#### Recovery blocks

#### Multiple-version programs

### System fault tolerance

At a third level, the computer subsystem may provide functions that compensate for failures in other system facilities that are not computer-based. This is system fault tolerance. For example, software can detect and compensate for failures in sensors. Measures at this level are usually application-specific. It is important that fault tolerance measures at all levels be compatible.

## Dependency relations

### Acyclic / Cyclic

A component of a system is said to **depend on another** component if the correctness of the first component's behavior requires the correct operation of the second component. Traditionally, the set of possible dependencies in a system are considered to form an acyclic graph. The term **fault tree** analysis seems to imply this, among other things. Indeed many systems exhibit this behavior, in which one fault leads to another which leads to another until eventually a failure occurs. It is possible, however, for a dependency relationship to cycle back upon itself. A dependency relationship is said to be **acyclic** if it forms part of a tree. A **cyclic** dependency relationship is one that cannot be described as part of a tree, but rather must be described as part of a directed cyclic graph.

### Failure regions

Defining a **failure region** limits the consideration of faults and failures to a portion of a system and its environment. This is necessary to insure that system specification, analysis and design efforts are concentrated on the portions of a system that can be observed and controlled by the designer and user. It helps to simplify an otherwise overwhelming task.

A system is typically made up of lots of components parts. These components are, in turn, made up of sub-components This continues arbitrarily until an **atomic component** (a component that is not divisible or that we choose not to divide into sub-components) is reached. Although all components are theoretically capable of having faults, for any system there is a level beyond which the faults are "not interesting". This level is called the **fault floor**. Atomic
components lie at the fault floor. **We are concerned with faults emerging from atomic components, but not faults that lie within these components**.

Similarly, as components are aggregated into a system, eventually the system is complete. Everything else (the user, the environment) is not a part of the system. This is the **system boundary**. **Failures occur when faults reach the system boundary**.

The **span of concern** begins at the boundaries between the system and the user and between the system and the environment, and ends at the fault floor. Faults below the fault floor are indistinguishable, either because they are not fully understood, or because they are too numerous. Informally, **the span of concern is the area within which faults are of interest**.

![](/images/fault-tolerance/span-of-concern.png)

## Fault classes

No system can be made to tolerate all possible faults, so it is essential that the faults be considered throughout the requirements definition and system design process. However, it is impractical to enumerate all of the faults to be tolerated; faults must be aggregated into manageable **fault classes**.

Faults may be classified based on:

* [Locality](#locality)
* [Effect](#effect)
* [Cause](#immediate-cause)
* [Duration](#duration)
* Effect on System State

### Locality

#### Atomic Component Faults

A atomic component fault is a fault at the fault floor, that is, in a component that cannot be subdivided for analysis purposes.

#### Composite Component Faults

A composite component fault is one that arises within an aggregation of atomic components rather than in an atomic component. It may be the result of one or more atomic component faults.

#### System Level Faults

A system level fault is one that arises in the structure of a system rather than in the system's components. Such faults are usually interaction or integration faults, that is, they occur because of the way the system is assembled rather than because of the integrity of any individual component. Note that an inconsistency in the operating rules for a system may lead to a system level fault. System level faults also include operator faults, in which an operator does not correctly perform his or her role in system operation.

Systems that distribute objects or information are prone to a special kind of system fault: **replication faults**. Replication faults occur when replicated information in a system becomes inconsistent, either because replicates that
are supposed to provide identical results no longer do so, or because the aggregate of the data from the various replicates is no longer consistent with system specifications.

Replication faults can be caused by **malicious faults**, in which components such as processors "lie" by providing conflicting versions of the same information to other components in the system. Malicious faults are sometimes called **[Byzantine faults](https://en.wikipedia.org/wiki/Byzantine_fault)** after an early formulation of the problem in terms of Byzantine generals trying to reach a consensus on attacking when one of the generals is a traitor.

#### External Faults

External faults arise from outside the system boundary, the environment, or the user. **Environmental faults** include phenomena that directly affect the operation of the system, such as temperature, vibration, or nuclear or electromagnetic radiation or that affect the inputs provided to the system. **User faults** are created by the user in employing the system. Note that the roles of user and operator are considered separately; the user is considered to be external to the system while the operator is considered to be a part of the system.

### Effect

Faults may also be classified according to their effect on the user of the system or service. Since computer system components interact by exchanging data values in a specified time and/or sequence, fault effects can be cleanly separated into timing faults and value faults. Timing faults occur when a value is delivered before or after the specified time. Value faults occur when the data differs in value from the specification.

#### Value Faults

Computer systems communicate by providing values. A value fault occurs when a computation returns a result that does not meet the system's specification. Value faults are usually detected using knowledge of the allowable values of the data, possibly determined at run time.

#### Timing Faults

A timing fault occurs when a process or service is not delivered or completed within the specified time interval. Timing faults cannot occur if there is no explicit or implicit specification of a deadline. Timing faults can be detected by observing the time at which a required interaction takes place; no knowledge of the data involved is usually needed.

Since time increases monotonically, it is possible to further classify timing faults into:

* early
* late
* "never" (omission)

Since it is practically impossible to determine if "never" occurs, omission faults are really late timing faults that exceed an arbitrary limit. Systems that never produce value faults, but only fail by omission are called **fail-silent systems**. If all failures require system restart, the system is a **fail-stop system**.

### Duration

#### Persistent faults

Persistent faults remain active for a significant period of time. These faults are sometimes termed hard faults. Persistent faults usually are the easiest to detect and diagnose, but may be difficult to contain and mask unless redundant hardware is available. Persistent faults can be effectively detected by test routines that are interleaved with normal processing.

#### Transient faults

Transient faults remain active for a short period of time. Because of their short duration, transient faults are often detected through the faults that result from their propagation.

#### Periodic faults

A transient fault that becomes active periodically is a periodic fault (sometimes referred to as an *intermittent fault*).

### Immediate (*direct*) Cause

Faults can be classified according to the operational condition that causes them.

#### Resource depletion faults

Resource depletion faults occur when a portion of the system is unable to obtain the resources required to perform its task. Resources may include time on a processing or communications device, storage, power, logical structures such as a data structure, or a physical item such as a processor.

#### Logic faults

Logic faults occur when adequate resources are available, but the system does not behave according to specification. Logic faults may be the result of improper design or implementation. Logic faults may occur in hardware or software.

#### Physical faults

Physical faults occur when hardware breaks or a mutation occurs in executable software. Most common fault tolerance mechanisms deal with hardware faults.

### Ultimate (*original*) Cause

Faults can also be classified as to their ultimate cause. Ultimate causes are the things that must be fixed to eliminate a fault. These faults occur during the development process and are most effectively dealt with using [fault avoidance](/notes/dependable-system#fault-avoidance) and [fault removal](/notes/dependable-system#fault-removal) techniques.

#### Specification fault

A common ultimate cause of a fault is an improper requirements specification which leads to a specification fault. Technically this is not a fault, since a fault is defined to be the failure of a component/interacting systems and a failure is the deviation of the system from specification. However, it can be the reason a system deviates from the behavior expected by the user. An especially insidious instance of this arises when the requirements ignore aspects of the environment in which the system operates. For instance, radiation causing a bit to flip in a memory location would be a *value fault* which would be considered an *external fault*. However, if the fault propagates inside the system boundary the ultimate cause is a *specification fault* because the system specification did not foresee the problem.

#### Design fault

Flowing down the waterfall, a design fault results when the system design does not correctly match the requirements. The validation process is specifically designed to detect these faults.

#### Implementation fault

Flowing down the waterfall, an implementation fault arises when the system implementation does not adequately implement the design. The validation process is specifically designed to detect these faults.

#### Documentation fault

A documentation fault occurs when the documented system does not match the real system.

### Effect on System State

#### Crash

#### Amnesia

#### Partial amnesia

## Fault Attributes

### Observability

Faults originate in a system component or subsystem, in the system's environment, or in an interaction between the system and a user, operator, or another subsystem. A fault may ultimately have one of several effects:

* It may disappear with no perceptible effect
* It may remain in place with no perceptible effect
* It may lead to a sequence of additional faults that result in a failure in the system's delivered service (propagation to failure)
* It may lead to a sequence of additional faults with no perceptible effect on the system (undetected propagation)
* It may lead to a sequence of additional faults that have a perceptible effect on the system but do not result in a failure in the system's delivered service (detected propagation without failure)

Fault detection is usually the first step in fault tolerance. Even if other elements of a system prevent a failure by compensating for a fault, it is important to detect and remove faults to avoid the exhaustion of a systems fault tolerance resources.

A fault is **observable** if there is information about its existence available at the system interface. The information that indicates the existence of a fault is a **symptom**. A symptom may be a directly observed fault or failure, or it may be a change in system behavior such that the system still meets its specifications. A fault that a fault tolerance mechanism of a system has found is said to be **detected**. Otherwise it is **latent**, whether it is observable or not. The definition of detected is independent of whether or not the fault tolerance mechanism is able to successfully deal with the fault condition. For a fault to be detected, it is sufficient that it be known about.

### Propagation

A fault that propagates to other faults or failures is said to be **active**. A non-propagating fault is said to be **dormant**. When a previously dormant fault becomes active it is said to be **triggered**. An active fault may again become dormant, awaiting a new trigger. The sequence of faults, each successive one triggered by the preceding one and possibly ending in a failure, is known as a **fault trajectory**. Because of the ways faults trigger successive faults, a fault trajectory could be viewed as a chain reaction.

![](/images/fault-tolerance/fault-attributes.png)

---

#### References

* Walter L. Heimerdinger, Charles B. Weinstock, "A Conceptual Framework for System Fault Tolerance";
